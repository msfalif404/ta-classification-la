{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification: Language Anxiety"
      ],
      "metadata": {
        "id": "Hn6Sp1gN_HH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Library"
      ],
      "metadata": {
        "id": "S3MPDHn4_LMr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UnOxFgzvRIO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU, Bidirectional, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D\n",
        "\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import keras\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading The Dataset"
      ],
      "metadata": {
        "id": "pg1FVcrW_O-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/dataset_2class_better.xlsx')\n",
        "df = df[['post','LA Classification']]\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Q-MegjxuwH7t",
        "outputId": "e1fd8e3f-e552-475c-e2a3-2ae415ec4229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index(['post', 'LA Classification'], dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-3921353925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/dataset_2class_better.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'LA Classification'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6249\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['post', 'LA Classification'], dtype='object')] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking Class Distribution"
      ],
      "metadata": {
        "id": "iEJSfwzq_Qua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengambil nilai perhitungan distribusi kelas\n",
        "class_counts = df['LA Classification'].value_counts()\n",
        "\n",
        "# Membuat diagram batang\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_counts.plot(kind='bar', color='skyblue')\n",
        "plt.title('Distribution of Classes')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)  # Untuk memutar label sumbu x agar lebih mudah dibaca\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HL00NXYHwUlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['LA Classification'].value_counts()"
      ],
      "metadata": {
        "id": "_0eRKHofx_jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encode Target Variable"
      ],
      "metadata": {
        "id": "1UtRaqH8_UoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['la_class'] = label_encoder.fit_transform(df['LA Classification'])\n",
        "\n",
        "# Menampilkan mapping kelas ke nilai encoded\n",
        "print(\"Mapping kelas ke nilai encoded:\")\n",
        "for i, class_label in enumerate(label_encoder.classes_):\n",
        "    print(f\"{class_label}: {i}\")\n",
        "\n",
        "df.drop(columns=['LA Classification'], inplace=True)"
      ],
      "metadata": {
        "id": "Q9ZyPBkmwgGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contraction Mapping"
      ],
      "metadata": {
        "id": "TtL5Wt8K5uW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"'cause\": \"because\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"couldn't've\": \"could not have\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hadn't've\": \"had not have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he had\",\n",
        "    \"he'd've\": \"he would have\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he'll've\": \"he will have\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'd'y\": \"how do you\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how does\",\n",
        "    \"I'd\": \"I had\",\n",
        "    \"I'd've\": \"I would have\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'll've\": \"I will have\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it had\",\n",
        "    \"it'd've\": \"it would have\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it'll've\": \"it will have\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"mightn't've\": \"might not have\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"mustn't've\": \"must not have\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"needn't've\": \"need not have\",\n",
        "    \"o'clock\": \"of the clock\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"oughtn't've\": \"ought not have\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"sha'n't\": \"shall not\",\n",
        "    \"shan't've\": \"shall not have\",\n",
        "    \"she'd\": \"she had\",\n",
        "    \"she'd've\": \"she would have\",\n",
        "    \"she'll\": \"she she will\",\n",
        "    \"she'll've\": \"she will have\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"shouldn't've\": \"should not have\",\n",
        "    \"so've\": \"so have\",\n",
        "    \"so's\": \"so is\",\n",
        "    \"that'd\": \"that had\",\n",
        "    \"that'd've\": \"that would have\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there'd\": \"there had\",\n",
        "    \"there'd've\": \"there would have\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they had\",\n",
        "    \"they'd've\": \"they would have\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they'll've\": \"they will have\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"to've\": \"to have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we had\",\n",
        "    \"we'd've\": \"we would have\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we'll've\": \"we will have\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what'll've\": \"what will have\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"when's\": \"when is\",\n",
        "    \"when've\": \"when have\",\n",
        "    \"where'd\": \"where did\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"where've\": \"where have\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who'll've\": \"who will have\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"why's\": \"why is\",\n",
        "    \"why've\": \"why have\",\n",
        "    \"will've\": \"will have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"won't've\": \"will not have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"wouldn't've\": \"would not have\",\n",
        "    \"y'all\": \"you all\",\n",
        "    \"y'all'd\": \"you all would\",\n",
        "    \"y'all'd've\": \"you all would have\",\n",
        "    \"y'all're\": \"you all are\",\n",
        "    \"y'all've\": \"you all have\",\n",
        "    \"you'd\": \"you had\",\n",
        "    \"you'd've\": \"you would have\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you'll've\": \"you will have\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contraction_mapping):\n",
        "    for contraction, expanded in contraction_mapping.items():\n",
        "        text = text.replace(contraction, expanded)\n",
        "    return text\n",
        "\n",
        "df['post'] = df['post'].apply(lambda x: expand_contractions(x, contraction_mapping))"
      ],
      "metadata": {
        "id": "vPOCdAsr5vp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning The Text"
      ],
      "metadata": {
        "id": "SausJ_HC5w_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.remove('not')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Hapus karakter non-alphanumeric\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # Konversi teks ke huruf kecil\n",
        "    text = text.lower()\n",
        "    # Split teks menjadi kata-kata\n",
        "    words = text.split()\n",
        "    # Lematisasi dan hapus stop words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(all_stopwords)]\n",
        "    # Gabungkan kata-kata kembali menjadi satu string\n",
        "    processed_text = ' '.join(words)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "df['post'] = df['post'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "TnsbWXRQ50aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Text Length"
      ],
      "metadata": {
        "id": "1RQUoVvh_WtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text_length'] = df['post'].apply(len)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zVjNvtrbwlAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting The Dataset Into Train And Test"
      ],
      "metadata": {
        "id": "WbYtTWtf__Uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['post'], df['la_class'], test_size = 0.2, stratify = df['la_class'], random_state = 42)"
      ],
      "metadata": {
        "id": "bB6ZftsZwvNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter"
      ],
      "metadata": {
        "id": "JxSiiaMcACWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = df['text_length'].max()\n",
        "trunc_type = 'post'\n",
        "padding_type = 'pre'\n",
        "oov_tok = '<OOV>'\n",
        "vocab_size = 500"
      ],
      "metadata": {
        "id": "f-CMGxq0w5G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing"
      ],
      "metadata": {
        "id": "BsLDIwZbBGE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size,\n",
        "                      char_level = False,\n",
        "                      oov_token = oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "-fG9tTCEw749"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get The Word Index"
      ],
      "metadata": {
        "id": "lBjhmGVkBO1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index)\n",
        "total_words"
      ],
      "metadata": {
        "id": "e8Dpr2_Yw_jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Train Sequences"
      ],
      "metadata": {
        "id": "wCoy2unOBVOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "training_padded = pad_sequences(training_sequences,\n",
        "                                maxlen = max_len,\n",
        "                                padding = padding_type,\n",
        "                                truncating = trunc_type)"
      ],
      "metadata": {
        "id": "hkne-4DgxBWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Test Sequences"
      ],
      "metadata": {
        "id": "5LFm3Em9BXN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded = pad_sequences(testing_sequences,\n",
        "                               maxlen = max_len,\n",
        "                               padding = padding_type,\n",
        "                               truncating = trunc_type)"
      ],
      "metadata": {
        "id": "45e6YMk2xDBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of training tensor: ', training_padded.shape)\n",
        "print('Shape of testing tensor: ', testing_padded.shape)"
      ],
      "metadata": {
        "id": "1_vwJgfUxGBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Architecture"
      ],
      "metadata": {
        "id": "v3gqfyJlBnxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                  classes=np.unique(y_train),\n",
        "                                                  y=y_train)\n",
        "\n",
        "# Definisikan nilai class weights sebagai dictionary\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "class_weight_dict"
      ],
      "metadata": {
        "id": "k2qIzZfK9wl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Paramter"
      ],
      "metadata": {
        "id": "7r6jEQVtBrvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 500\n",
        "embedding_dim = 16\n",
        "drop_value = 0.2"
      ],
      "metadata": {
        "id": "Bb5eY9hfxHr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Architecture"
      ],
      "metadata": {
        "id": "mKwGxBw1gf0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter\n",
        "n_lstm = 32\n",
        "drop_lstm = 0.2\n",
        "# Define LSTM Model\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
        "model1.add(SpatialDropout1D(drop_lstm))\n",
        "model1.add(LSTM(n_lstm, return_sequences=False))\n",
        "model1.add(Dropout(drop_lstm))\n",
        "model1.add(Dense(1, activation='tanh'))"
      ],
      "metadata": {
        "id": "ux-cqfCu0Sz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "id": "XcqtLkGi0U-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.AdamW()\n",
        "\n",
        "model1.compile(loss = 'binary_crossentropy',\n",
        "               optimizer = optimizer,\n",
        "               metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "H_l2BFwz0W7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 11\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=8)\n",
        "history = model1.fit(training_padded,\n",
        "                     y_train,\n",
        "                     epochs=num_epochs,\n",
        "                     validation_data=(testing_padded, y_test),\n",
        "                     class_weight=class_weight_dict,\n",
        "                    #  callbacks =[early_stop],\n",
        "                     verbose=1)"
      ],
      "metadata": {
        "id": "Xyh_cp2U0Yhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WeyFgq7C0uQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Prediksi probabilitas\n",
        "y_pred_prob = model1.predict(testing_padded)\n",
        "\n",
        "# Mengubah probabilitas menjadi kelas biner\n",
        "y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Tampilkan classification report\n",
        "print(classification_report(y_test, y_pred_binary))"
      ],
      "metadata": {
        "id": "0-XSLha4A4OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract The Model After Saving The Model"
      ],
      "metadata": {
        "id": "wkH61GeJDpCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# import os\n",
        "\n",
        "# # Path ke file .zip yang diunggah\n",
        "# zip_file_path = \"/content/lstm_model.zip\"\n",
        "\n",
        "# # Path ke folder ekstraksi\n",
        "# extract_folder_path = \"/content/\"\n",
        "\n",
        "# # Buat folder ekstraksi jika belum ada\n",
        "# if not os.path.exists(extract_folder_path):\n",
        "#     os.makedirs(extract_folder_path)\n",
        "\n",
        "# # Ekstrak file .zip\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_folder_path)"
      ],
      "metadata": {
        "id": "NDSnfoxgDr-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preding The Model Using Loaded Model"
      ],
      "metadata": {
        "id": "ilZkAfRDCtph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "\n",
        "# # Muat model dari folder yang telah diekstrak\n",
        "# model_folder_path = \"/content/lstm_model\"\n",
        "# loaded_model = tf.keras.models.load_model(model_folder_path)\n",
        "\n",
        "# # Prediksi probabilitas\n",
        "# y_pred_prob = loaded_model.predict(testing_padded)\n",
        "\n",
        "# # Mengubah probabilitas menjadi kelas biner\n",
        "# y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# # Hitung dan tampilkan akurasi\n",
        "# accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "# print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# # Hitung dan tampilkan skor ROC-AUC\n",
        "# roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "# print(f\"ROC-AUC Score: {roc_auc}\")\n",
        "\n",
        "# # Tampilkan classification report\n",
        "# print(classification_report(y_test, y_pred_binary))"
      ],
      "metadata": {
        "id": "8DTPmOnfCwcA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}